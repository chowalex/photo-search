{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chowalex/photo-search/blob/main/Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcGjIPo-VWR-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYIJu0wOoXWf"
      },
      "source": [
        "This is a demo of indexing a collection of images with CLIP, enabling semantic search of those images. Specifically it does the following:\n",
        "1. Pulls images from an Azure blob storage container\n",
        "2. Computes embeddings of each image with CLIP\n",
        "3. Saves vectors in Pinecone\n",
        "4. Computes the embedding for a text query, retrieves the top-K results from Pinecone, and displays those images\n",
        "\n",
        "# Prerequisites\n",
        "These secrets must be set as environment variables in the notebook:\n",
        "* `HF_TOKEN`: Hugging Face token\n",
        "* `PINECONE_API_KEY`: Pinecone API Key\n",
        "* `AZURE_CONNECTION_STRING`: Connection string for the Azure blob storage container\n",
        "\n",
        "In addition, set these constants based on the specific Azure and Pinecone setup:\n",
        "* `CONTAINER_NAME`\n",
        "* `PINECONE_INDEX`\n",
        "* `PINECONE_NAMESPACE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hrvrJNGjWQAe",
        "outputId": "833821e9-c9ff-4afa-ec85-f5724c2f544d"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client\n",
        "!pip install azure-storage-blob\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "from io import BytesIO\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hkErMtWBbJTw",
        "outputId": "947683e8-1891-43de-85a0-4ec04b6abccd"
      },
      "outputs": [],
      "source": [
        "# Set up CLIP model and parameters\n",
        "DIMENSION = 512\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "# Move model to device if possible\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bs4AScdeaiWX"
      },
      "outputs": [],
      "source": [
        "# Azure and Pinecone settings\n",
        "CONNECTION_STRING = userdata.get('AZURE_CONNECTION_STRING')\n",
        "CONTAINER_NAME = 'search'\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_INDEX = 'photo-search'\n",
        "PINECONE_NAMESPACE = 'clip-demo'\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# Initialize the connection to Azure Blob Storage\n",
        "if CONNECTION_STRING is None:\n",
        "    raise ValueError(\"AZURE_CONNECTION_STRING environment variable not set.\")\n",
        "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\n",
        "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
        "\n",
        "# Set up pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Create the index if it does not already exist.\n",
        "for index in pc.list_indexes():\n",
        "  if index['name'] == PINECONE_INDEX:\n",
        "    break\n",
        "else:\n",
        "  pc.create_index(\n",
        "    name=PINECONE_INDEX,\n",
        "    dimension=DIMENSION,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(PINECONE_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lqrZGzZEklAq",
        "outputId": "30ba912f-0c70-4d0b-926c-bd94979f8b49"
      },
      "outputs": [],
      "source": [
        "blob_list = container_client.list_blobs()\n",
        "blob_names = [blob.name for blob in blob_list]\n",
        "num_images = len(blob_names)\n",
        "print(f\"Found {num_images} blobs in the container.\")\n",
        "\n",
        "def download_blob_as_image(blob_name):\n",
        "  blob_client = container_client.get_blob_client(blob_name)\n",
        "  blob_data = blob_client.download_blob().readall()\n",
        "  image = Image.open(BytesIO(blob_data))\n",
        "  return image\n",
        "\n",
        "def get_embeddings_for_batch(blob_names):\n",
        "  images = []\n",
        "  for blob_name in blob_names:\n",
        "    images.append(download_blob_as_image(blob_name))\n",
        "\n",
        "  images_tokens = processor(\n",
        "      text=None,\n",
        "      images=images,\n",
        "      return_tensors='pt'\n",
        "  )['pixel_values'].to(device)\n",
        "\n",
        "  images_tokens.shape\n",
        "  images_emb = model.get_image_features(images_tokens)\n",
        "\n",
        "  # Normalize embeddings\n",
        "  images_emb = images_emb.detach().cpu().numpy()\n",
        "  images_emb_norm = images_emb.T / np.linalg.norm(images_emb, axis=1)\n",
        "  images_emb_norm = images_emb_norm.T\n",
        "  images_emb_norm.shape\n",
        "  return images_emb_norm\n",
        "\n",
        "for i in range(0, num_images, BATCH_SIZE):\n",
        "  blob_batch = blob_names[i : i+BATCH_SIZE]\n",
        "  images_emb_norm = get_embeddings_for_batch(blob_batch)\n",
        "  print(f\"Successfully processed batch with normalized embeddings {images_emb_norm.shape}. Min: {images_emb_norm.min()}, Max: {images_emb_norm.max()}. Batch: {blob_batch}\")\n",
        "\n",
        "  # Save embeddings to pinecone in batch.\n",
        "  # The vector ID = blob name.\n",
        "  vectors = []\n",
        "  for j, emb in enumerate(images_emb_norm):\n",
        "    idx = i + j\n",
        "    vectors.append({\"id\": blob_names[idx], \"values\": emb})\n",
        "\n",
        "  index.upsert(\n",
        "      vectors = vectors,\n",
        "      namespace = PINECONE_NAMESPACE\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vTaUrLtMQSLv",
        "outputId": "ecd36274-922d-4768-c467-08bb3b267c70"
      },
      "outputs": [],
      "source": [
        "query = \"aquarium\"\n",
        "\n",
        "tokens = processor(\n",
        "    text=[query],\n",
        "    padding=True,\n",
        "    images=None,\n",
        "    return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "text_emb = model.get_text_features(**tokens)\n",
        "text_emb = text_emb.detach().cpu().numpy()\n",
        "\n",
        "# Normalize\n",
        "norm_factor = np.linalg.norm(text_emb, axis=1)\n",
        "norm_factor.shape\n",
        "text_emb_norm = text_emb.T / norm_factor\n",
        "text_emb_norm = text_emb_norm.T\n",
        "print(f\"Computed normalized text embedding {text_emb.shape}\")\n",
        "\n",
        "# Query pinecone\n",
        "vector = text_emb_norm.flatten().tolist()\n",
        "response = index.query(vector=vector, namespace=PINECONE_NAMESPACE, top_k=2)\n",
        "print(response)\n",
        "if 'matches' in response:\n",
        "  for match in response['matches']:\n",
        "    blob_name = match['id']\n",
        "    display(download_blob_as_image(blob_name))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMKCqogrS3Qjk43Duk74Eyi",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1qZYvXso97vFxXH5Ck_0lbQmqcBfCXg6N",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
